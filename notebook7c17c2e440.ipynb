{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"[BeautifulSoup](https://pypi.org/project/beautifulsoup4/) \n\n**Version:** 0.5\n<p>**Description:** Adding the Banks (Angola) MX Records to the dataset</p>\n\nThis code uses the dns.resolver module from dnspython to retrieve the MX records for each naked domain. The MX records are stored in the \"mx_records\" column of the CSV file.\n\nThis updated code handles common exceptions related to DNS resolution, such as NXDOMAIN, NoNameservers, and NoAnswer. Additionally, it captures any other DNSException and includes the error message in the mx_records field.","metadata":{}},{"cell_type":"code","source":"import csv\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urlparse\nimport dns.resolver\nimport requests\n\n# Send a GET request to the webpage\nurl = \"https://www.abanc.ao/sistema-financeiro/instituicoes-bancarias-autorizadas/\"\nresponse = requests.get(url)\n\n# Create a BeautifulSoup object to parse the HTML content\nsoup = BeautifulSoup(response.content, \"html.parser\")\n\n# Find all bank information div elements\nbank_divs = soup.find_all(\"div\", class_=\"cmsAccordion\")\n\n# Initialize an empty list to store the bank details\nbank_details = []\n\n# Iterate over each bank div\nfor bank_div in bank_divs:\n    # Extract bank name\n    bank_name = bank_div.find(\"p\", class_=\"heading\").text.strip()\n\n    # Extract bank info divs\n    bank_info_divs = bank_div.find_all(\"div\", class_=\"content\")\n\n    # Extract bank website\n    bank_website = None\n    for bank_info_div in bank_info_divs:\n        bank_website_element = bank_info_div.find(\"a\")\n        if bank_website_element:\n            bank_website = bank_website_element[\"href\"]\n            break\n\n    # If bank website is not found, check the second occurrence of class \"content\"\n    if not bank_website and len(bank_info_divs) > 1:\n        bank_info_div = bank_info_divs[1]\n        bank_website_element = bank_info_div.find(\"a\")\n        if bank_website_element:\n            bank_website = bank_website_element[\"href\"]\n\n    # Extract the naked domain from the bank website URL\n    naked_domain = None\n    if bank_website:\n        parsed_url = urlparse(bank_website)\n        netloc = parsed_url.netloc\n        if netloc.startswith(\"www.\"):\n            naked_domain = netloc[4:]  # Remove \"www\" part from the domain\n        else:\n            naked_domain = netloc\n\n    # Retrieve MX records for the naked domain\n    mx_records = None\n    if naked_domain:\n        try:\n            answers = dns.resolver.resolve(naked_domain, \"MX\")\n            mx_records = [str(r.exchange)[:-1] for r in answers]\n        except (dns.resolver.NXDOMAIN, dns.resolver.NoNameservers, dns.resolver.NoAnswer):\n            mx_records = \"No MX records found\"\n        except dns.exception.DNSException as e:\n            mx_records = f\"Error: {str(e)}\"\n\n    # Create a dictionary to store the bank details\n    bank = {\n        \"name\": bank_name,\n        \"website\": bank_website,\n        \"naked_domain\": naked_domain,\n        \"mx_records\": mx_records\n    }\n\n    # Append the bank details to the list\n    bank_details.append(bank)\n\n# Specify the output CSV file path\ncsv_file = \"bank_details-0.5.csv\"\n\n# Write the bank details to the CSV file\nwith open(csv_file, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n    writer = csv.DictWriter(file, fieldnames=[\"name\", \"website\", \"naked_domain\", \"mx_records\"])\n    writer.writeheader()\n    writer.writerows(bank_details)\n\nprint(\"Bank details written to\", csv_file)","metadata":{},"execution_count":null,"outputs":[]}]}